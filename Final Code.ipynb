{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e456f740-f57d-49ab-b78e-e76c2b6af033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Adapted from https://github.com/TianhongDai/hindsight-experience-replay\n",
    "#and https://github.com/openai/baselines/tree/master/baselines/her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3351b225-05a3-4b8e-ab98-56af6a6032fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Networks.py\n"
     ]
    }
   ],
   "source": [
    "%%file Networks.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256,action_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.tanh(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim,action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256,1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, actions):\n",
    "        x = torch.cat([x, actions], dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        q = self.fc4(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293c7b97-2686-49f1-9747-82e88a1fa6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HER_ReplayBuffer.py\n"
     ]
    }
   ],
   "source": [
    "%%file HER_ReplayBuffer.py\n",
    "import threading\n",
    "import numpy as np\n",
    "\n",
    "class HER_ReplayBuffer:\n",
    "    #Replay Buffer code adapted from https://github.com/openai/baselines/blob/master/baselines/her/replay_buffer.py\n",
    "    #Sampler code adapted from https://github.com/openai/baselines/blob/master/baselines/her/her_sampler.py\n",
    "    def __init__(self, buffer_size, state_dim, goal_dim, action_dim,max_steps,reward_function = None, strategy = 'Method3'):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_dim = state_dim\n",
    "        self.goal_dim = goal_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.max_steps = max_steps\n",
    "        self.max_ep_in_buff = int(self.buffer_size / self.max_steps)\n",
    "        self.reward_function = reward_function\n",
    "        \n",
    "        self.current_size = 0\n",
    "        self.buffer = {'state': np.empty([self.max_ep_in_buff, self.max_steps + 1, self.state_dim]),\n",
    "                       'achieved_goal':np.empty([self.max_ep_in_buff,self.max_steps+1, self.goal_dim]),\n",
    "                       'desired_goal':np.empty([self.max_ep_in_buff,self.max_steps,self.goal_dim]),\n",
    "                       'action':np.empty([self.max_ep_in_buff,self.max_steps,self.action_dim])\n",
    "                      }\n",
    "        self.strategy = strategy\n",
    "        self.epsilon = 0.3\n",
    "        \n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def add_replay(self,episode):\n",
    "        roll = False\n",
    "        if self.current_size + 1 > self.buffer_size:\n",
    "            roll = True\n",
    "        else:\n",
    "            self.current_size += 1\n",
    "        for ind,key in enumerate(self.buffer.keys()):\n",
    "            #Roll each buffer entry to the left, insert new entry at end if buffer is full\n",
    "            if roll:\n",
    "                self.buffer[key] = np.roll(self.buffer[key],-1,axis = 0)\n",
    "            self.buffer[key][self.current_size - 1] = episode[ind]\n",
    "    \n",
    "    def sample(self,batch_size,temp_buffer = None,k = 4):\n",
    "        her_ratio = 1 - (1/(1+k))\n",
    "        if temp_buffer:\n",
    "            current_size = temp_buffer['actions'].shape[0]\n",
    "            max_steps = temp_buffer['actions'].shape[1]\n",
    "        else:\n",
    "            temp_buffer = {}\n",
    "            for key in self.buffer.keys():\n",
    "                temp_buffer[key] = self.buffer[key][:self.current_size]\n",
    "            temp_buffer['next_state'] = temp_buffer['state'][:,1:,:]\n",
    "            temp_buffer['next_agoal'] = temp_buffer['achieved_goal'][:,1:,:]\n",
    "            current_size = self.current_size \n",
    "            max_steps = self.max_steps\n",
    "        #Sampling Using HER 'future' strategy\n",
    "        episode_inds = np.random.randint(0,current_size,size = batch_size)\n",
    "        transition_inds = np.random.randint(max_steps,size = batch_size)\n",
    "        transitions = {key: temp_buffer[key][episode_inds, transition_inds]\\\n",
    "                       for key in temp_buffer.keys()}\n",
    "        #Sampling k HER transitions for every non HER transition. \n",
    "        her_inds = np.where(np.random.uniform(size=batch_size) < her_ratio)\n",
    "        \n",
    "        if (self.strategy == 'Vanilla'):\n",
    "            new_goal_inds = np.random.randint(transition_inds[her_inds]+1,max_steps+1)\n",
    "        else:\n",
    "            #Sample new desired goal inds weighted by distance to actual goal\n",
    "            agoal_inds = np.array([np.arange(start+1,max_steps+1) for start in transition_inds[her_inds]],dtype = object)\n",
    "            agoals = np.array([temp_buffer['achieved_goal'][e_ind,g_ind] for e_ind,g_ind in zip(episode_inds,agoal_inds)],dtype=object)\n",
    "            dgoals = temp_buffer['desired_goal'][episode_inds,0]\n",
    "\n",
    "            dists = np.array([np.sqrt(np.sum(np.square(ag-dg),axis = 1)) for ag,dg in zip(agoals,dgoals)],dtype = object)\n",
    "            if (self.strategy == 'Method1'):\n",
    "                new_goal_inds = np.array([ag_inds[np.argmax(d)] for ag_inds,d in zip(agoal_inds,dists)])       \n",
    "            elif (self.strategy == 'Method2'):\n",
    "                new_goal_inds = np.array([ag_inds[np.argmax(d)] \\\n",
    "                                          if np.random.rand() > self.epsilon else ag_inds[np.random.randint(0,len(ag_inds))] \\\n",
    "                                          for ag_inds,d in zip(agoal_inds,dists)])     \n",
    "            elif (self.strategy == 'Method3'):\n",
    "                weights = np.array([d/sum(d) for d in dists],dtype = object)\n",
    "                new_goal_inds = np.array([np.random.choice(ag_inds,p=w) for ag_inds,w in zip(agoal_inds,weights)])     \n",
    "            else:\n",
    "                raise ValueError('Strategy {} does not exist'.format(self.strategy))\n",
    "        \n",
    "        transitions['desired_goal'][her_inds] = temp_buffer['achieved_goal'][episode_inds[her_inds],new_goal_inds]\n",
    "        \n",
    "        transitions['reward'] = self.reward_function(transitions['next_agoal'],transitions['desired_goal'],None)\n",
    "        transitions['reward'] = np.expand_dims(transitions['reward'],1)\n",
    "        # transitions = {k: transitions[k].reshape(batch_size, *transitions[k].shape[1:]) for k in transitions.keys()}\n",
    "        return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50043760-4701-4995-8574-1310258473d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Normalizer.py\n"
     ]
    }
   ],
   "source": [
    "%%file Normalizer.py\n",
    "import threading\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self,dim_size,eps=1e-2,clip_range = np.inf):\n",
    "        self.dim_size = dim_size #Dimensional size of the input\n",
    "        self.eps = eps\n",
    "        self.clip_range = clip_range\n",
    "        \n",
    "        #Local sum of each process\n",
    "        self.local_sum = np.zeros(self.dim_size, np.float32)\n",
    "        self.local_sumsq = np.zeros(self.dim_size, np.float32)\n",
    "        self.local_count = np.zeros(1, np.float32)\n",
    "\n",
    "        self.total_sum = np.zeros(self.dim_size, np.float32)\n",
    "        self.total_sumsq = np.zeros(self.dim_size, np.float32)\n",
    "        self.total_count = np.ones(1, np.float32)\n",
    "        \n",
    "        # get the mean and std\n",
    "        self.mean = np.zeros(self.dim_size, np.float32)\n",
    "        self.std = np.ones(self.dim_size, np.float32)\n",
    "        \n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def update_local(self,input_arr):\n",
    "        input_arr = input_arr.reshape(-1, self.dim_size)\n",
    "\n",
    "        with self.lock:\n",
    "            self.local_sum += input_arr.sum(axis=0)\n",
    "            self.local_sumsq += (np.square(input_arr)).sum(axis=0)\n",
    "            self.local_count[0] += input_arr.shape[0]\n",
    "    \n",
    "    def sync(self, local_sum, local_sumsq, local_count):\n",
    "        local_sum[...] = self.mpi_sum(local_sum)\n",
    "        local_sumsq[...] = self.mpi_sum(local_sumsq)\n",
    "        local_count[...] = self.mpi_sum(local_count)\n",
    "        return local_sum, local_sumsq, local_count\n",
    "\n",
    "    def mpi_sum(self, x):\n",
    "        buf = np.zeros_like(x)\n",
    "        MPI.COMM_WORLD.Allreduce(x, buf, op=MPI.SUM)\n",
    "        buf /= MPI.COMM_WORLD.Get_size()\n",
    "        return buf\n",
    "    \n",
    "    def update_all(self):\n",
    "        sync_sum, sync_sumsq, sync_count = self.sync(self.local_sum,\\\n",
    "                                                     self.local_sumsq, \\\n",
    "                                                     self.local_count)\n",
    "\n",
    "        with self.lock:\n",
    "            self.local_sum = self.local_sum * 0\n",
    "            self.local_sumsq = self.local_sumsq * 0\n",
    "            self.local_count = self.local_count * 0\n",
    "\n",
    "        self.total_sum += sync_sum\n",
    "        self.total_sumsq += sync_sumsq\n",
    "        self.total_count += sync_count\n",
    "        \n",
    "        self.mean = self.total_sum/self.total_count\n",
    "        self.std = np.sqrt(np.maximum(np.square(self.eps), \\\n",
    "                    (self.total_sumsq / self.total_count) - \\\n",
    "                    np.square(self.total_sum / self.total_count)))\n",
    "        \n",
    "    def normalize(self,input_arr):\n",
    "        return np.clip((input_arr-self.mean)/self.std, -self.clip_range, self.clip_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d5dc35-2308-4204-919d-d0e31d7b56d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mpi_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%file mpi_utils.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# sync_networks across the different cores\n",
    "def sync_networks(network):\n",
    "    \"\"\"\n",
    "    netowrk is the network you want to sync\n",
    "    \"\"\"\n",
    "    comm = MPI.COMM_WORLD\n",
    "    flat_params = _get_flat_params_or_grads(network, mode='params')\n",
    "    comm.Bcast(flat_params, root=0)\n",
    "    # set the flat params back to the network\n",
    "    _set_flat_params_or_grads(network, flat_params, mode='params')\n",
    "\n",
    "def sync_grads(network):\n",
    "    flat_grads = _get_flat_params_or_grads(network, mode='grads')\n",
    "    comm = MPI.COMM_WORLD\n",
    "    global_grads = np.zeros_like(flat_grads)\n",
    "    comm.Allreduce(flat_grads, global_grads, op=MPI.SUM)\n",
    "    _set_flat_params_or_grads(network, global_grads, mode='grads')\n",
    "\n",
    "# get the flat grads or params\n",
    "def _get_flat_params_or_grads(network, mode='params'):\n",
    "    \"\"\"\n",
    "    include two kinds: grads and params\n",
    "    \"\"\"\n",
    "    attr = 'data' if mode == 'params' else 'grad'\n",
    "    return np.concatenate([getattr(param, attr).cpu().numpy().flatten() for param in network.parameters()])\n",
    "\n",
    "def _set_flat_params_or_grads(network, flat_params, mode='params'):\n",
    "    \"\"\"\n",
    "    include two kinds: grads and params\n",
    "    \"\"\"\n",
    "    attr = 'data' if mode == 'params' else 'grad'\n",
    "    # the pointer\n",
    "    pointer = 0\n",
    "    for param in network.parameters():\n",
    "        getattr(param, attr).copy_(torch.tensor(flat_params[pointer:pointer + param.data.numel()]).view_as(param.data))\n",
    "        pointer += param.data.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97c47978-bffb-4205-b0b5-e64fc53e1f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DDPG_HER.py\n"
     ]
    }
   ],
   "source": [
    "%%file DDPG_HER.py\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy \n",
    "from mpi4py import MPI\n",
    "from mpi_utils import sync_networks, sync_grads\n",
    "from HER_ReplayBuffer import HER_ReplayBuffer\n",
    "from Networks import Actor, Critic\n",
    "from Normalizer import Normalizer\n",
    "\n",
    "class DDPG_HER:\n",
    "    def __init__(self,env,num_epochs = 50,strategy='vanilla',use_cuda = False):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_cycles = 50\n",
    "        self.num_episodes = 2\n",
    "        self.num_updates = 40\n",
    "        self.prob_rand_action = 0.2\n",
    "        self.buffer_size = int(1e6)\n",
    "        self.batch_size = 256\n",
    "        self.gamma = 0.98\n",
    "        self.lr = 0.001\n",
    "        self.tau = 0.05 #(1-decay_rate, decay_rate = 0.95)\n",
    "        self.observation_limit = 200\n",
    "        self.clip_range = 5\n",
    "        self.max_steps = env._max_episode_steps\n",
    "        self.noise_eps = 0.2 #Noise std\n",
    "        self.random_eps = 0.3 #Probability of choosing a random action\n",
    "        self.action_max = env.action_space.high[0]\n",
    "        self.l2_lambda = 1\n",
    "        self.env = env\n",
    "        \n",
    "        self.state_dim = env.observation_space['observation'].shape[0]\n",
    "        self.goal_dim = env.observation_space['achieved_goal'].shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        \n",
    "        self.device = 'cuda' if use_cuda else 'cpu'\n",
    "        \n",
    "        self.actor = Actor(self.state_dim+self.goal_dim,self.action_dim).to(self.device)\n",
    "        self.actor_target = copy.deepcopy(self.actor).to(self.device)\n",
    "        \n",
    "        self.critic = Critic(self.state_dim+self.goal_dim,self.action_dim).to(self.device)\n",
    "        self.critic_target = copy.deepcopy(self.critic).to(self.device)        \n",
    "        \n",
    "        sync_networks(self.actor)\n",
    "        sync_networks(self.critic)\n",
    "        \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = self.lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr = self.lr)\n",
    "        \n",
    "        self.replay_buffer = HER_ReplayBuffer(self.buffer_size, self.state_dim, self.goal_dim, \\\n",
    "                                              self.action_dim,self.max_steps,self.env.compute_reward)\n",
    "        \n",
    "        self.state_normalizer = Normalizer(self.state_dim,clip_range = self.clip_range)\n",
    "        self.goal_normalizer = Normalizer(self.goal_dim,clip_range = self.clip_range)\n",
    "        \n",
    "        \n",
    "        self.actor_loss_arr = []\n",
    "        self.critic_loss_arr = []\n",
    "        self.success_rate_arr = []\n",
    "        \n",
    "    def update_target_networks(self):\n",
    "        for target_param, source_param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_((self.tau * source_param.data) + ((1-self.tau) * target_param.data))\n",
    "        for target_param, source_param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_((self.tau * source_param.data) + ((1-self.tau) * target_param.data))\n",
    "    \n",
    "    def preprocess_inputs(self,state,goal):\n",
    "        state_norm = self.state_normalizer.normalize(state)\n",
    "        goal_norm = self.goal_normalizer.normalize(goal)\n",
    "        processed_input = torch.tensor(np.concatenate([state_norm,goal_norm], axis = -1),\\\n",
    "                                       dtype = torch.float32).to(self.device)#.unsqueeze(0)\n",
    "        return processed_input\n",
    "    \n",
    "    def select_action(self,state_input):\n",
    "        #Epsilon-Greedy Strategy\n",
    "        if np.random.rand() <= self.random_eps:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = self.actor(state_input).cpu().numpy().squeeze()\n",
    "        #add noise\n",
    "        noisy_action = action + np.random.normal(0,self.noise_eps,size = self.action_dim)\n",
    "        noisy_action = np.clip(action,-self.action_max,self.action_max)\n",
    "        return noisy_action\n",
    "    \n",
    "    def unpack_observation(self,observation):\n",
    "        state = observation['observation']\n",
    "        a_goal = observation['achieved_goal']\n",
    "        d_goal = observation['desired_goal']\n",
    "        return state,a_goal,d_goal\n",
    "    \n",
    "    def update_normalizer(self, episode):\n",
    "        state_arr, a_goal_arr, d_goal_arr, action_arr = episode\n",
    "        next_state_arr = state_arr[:, 1:, :]\n",
    "        next_agoal_arr = a_goal_arr[:, 1:, :]\n",
    "        # get the number of normalization transitions\n",
    "        num_transitions = action_arr.shape[1]\n",
    "        # create the new buffer to store them\n",
    "        temp_buffer = {'state': state_arr, \n",
    "                       'achieved_goal': a_goal_arr,\n",
    "                       'desired_goal': d_goal_arr, \n",
    "                       'actions': action_arr, \n",
    "                       'state_next': next_state_arr,\n",
    "                       'next_agoal': next_agoal_arr,\n",
    "                       }\n",
    "        transitions = self.replay_buffer.sample(num_transitions, \\\n",
    "                                                temp_buffer = temp_buffer)\n",
    "        state, g = transitions['state'], transitions['desired_goal']\n",
    "        # pre process the obs and g\n",
    "        clipped_curr_state_arr = np.clip(transitions['state'],-self.observation_limit, self.observation_limit)\n",
    "        clipped_d_goal_arr = np.clip(transitions['desired_goal'],-self.observation_limit, self.observation_limit)\n",
    "        # update\n",
    "        self.state_normalizer.update_local(clipped_curr_state_arr)\n",
    "        self.goal_normalizer.update_local(clipped_d_goal_arr)\n",
    "        # recompute the stats\n",
    "        self.state_normalizer.update_all()\n",
    "        self.goal_normalizer.update_all()\n",
    "        \n",
    "    def update_network(self):\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        curr_state_arr = np.clip(transitions['state'],-self.observation_limit, self.observation_limit)\n",
    "        next_state_arr = np.clip(transitions['next_state'],-self.observation_limit, self.observation_limit)\n",
    "        d_goal_arr = np.clip(transitions['desired_goal'],-self.observation_limit, self.observation_limit)\n",
    "        processed_curr_states = self.preprocess_inputs(curr_state_arr,d_goal_arr)\n",
    "        processed_next_states = self.preprocess_inputs(next_state_arr,d_goal_arr)\n",
    "        action_arr = torch.tensor(transitions['action'],\\\n",
    "                                 dtype = torch.float32).to(self.device)\n",
    "        reward_arr = torch.tensor(transitions['reward'],\\\n",
    "                                 dtype = torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            next_action_arr = self.actor_target(processed_next_states)\n",
    "            y = reward_arr + self.gamma * self.critic_target(processed_next_states,next_action_arr).detach()\n",
    "            y.detach()\n",
    "            #Clip Q value\n",
    "            y = torch.clamp(y,-1/(1-self.gamma),0)\n",
    "            \n",
    "        critic_loss = torch.mean((y - self.critic(processed_curr_states,action_arr))**2)\n",
    "        \n",
    "        actions = self.actor(processed_curr_states)\n",
    "        actor_loss = -torch.mean(self.critic(processed_curr_states,actions))\n",
    "        #L2 Regularlization\n",
    "        actor_loss += self.l2_lambda * torch.mean((actions/self.action_max)**2)\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        sync_grads(self.actor)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        sync_grads(self.critic)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        self.critic_loss_arr.append(float(critic_loss))\n",
    "        self.actor_loss_arr.append(float(actor_loss))\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for cycle in range(self.num_cycles):\n",
    "                state_arr, a_goal_arr, d_goal_arr, action_arr = ([[] for _ in range(self.num_episodes)] for _ in range(4))\n",
    "                for episode in range(self.num_episodes):\n",
    "                    observation = self.env.reset()\n",
    "                    curr_state, a_goal, d_goal = self.unpack_observation(observation)\n",
    "                    for step in range(self.max_steps):\n",
    "                        with torch.no_grad():\n",
    "                            processed_state = self.preprocess_inputs(curr_state,d_goal)\n",
    "                            action = self.select_action(processed_state)\n",
    "                            # print('p_state',processed_state,'action',action)\n",
    "                            # return\n",
    "                        next_observation,_,_,info = self.env.step(action)\n",
    "                        next_state,next_a_goal, _ = self.unpack_observation(next_observation)\n",
    "\n",
    "                        state_arr[episode].append(curr_state)\n",
    "                        a_goal_arr[episode].append(a_goal)\n",
    "                        d_goal_arr[episode].append(d_goal)\n",
    "                        action_arr[episode].append(action)\n",
    "\n",
    "                        curr_state = next_state\n",
    "                        a_goal = next_a_goal\n",
    "\n",
    "                    state_arr[episode].append(curr_state)\n",
    "                    a_goal_arr[episode].append(a_goal)\n",
    "                \n",
    "                state_arr = np.array(state_arr)\n",
    "                a_goal_arr = np.array(a_goal_arr)\n",
    "                d_goal_arr = np.array(d_goal_arr)\n",
    "                action_arr = np.array(action_arr)\n",
    "                \n",
    "                for states,agoals,dgoals,actions in zip(state_arr,a_goal_arr,d_goal_arr,action_arr):\n",
    "                    self.replay_buffer.add_replay([states, agoals, dgoals, actions])\n",
    " \n",
    "                self.update_normalizer([state_arr,a_goal_arr,d_goal_arr,action_arr])\n",
    "                    \n",
    "                for batch in range(self.num_updates):\n",
    "                    self.update_network()\n",
    "                self.update_target_networks()\n",
    "            self.success_rate_arr.append(self.test())\n",
    "            if MPI.COMM_WORLD.Get_rank() == 0:\n",
    "                print('Epoch:',epoch,\\\n",
    "                      'Avg Critic Loss:',np.mean(self.critic_loss_arr[-self.num_cycles*self.num_updates:]),\\\n",
    "                      'Avg Actor Loss:',np.mean(self.actor_loss_arr[-self.num_cycles*self.num_updates:]),\\\n",
    "                      'Success Rate:',np.mean(self.success_rate_arr[-self.num_cycles:]))\n",
    "\n",
    "    def test(self):\n",
    "        num_tests = 10\n",
    "        success_rate = 0\n",
    "        for i in range(num_tests):\n",
    "            observation = self.env.reset()\n",
    "            curr_state,_,d_goal = self.unpack_observation(observation)\n",
    "            done = False\n",
    "            while not done:\n",
    "                with torch.no_grad():\n",
    "                    processed_state = self.preprocess_inputs(curr_state,d_goal)\n",
    "                    action = self.actor(processed_state).detach().cpu().numpy().squeeze()\n",
    "                next_observation, _, done, info = self.env.step(action)\n",
    "                next_state,_,_ = self.unpack_observation(next_observation)\n",
    "                curr_state = next_state\n",
    "            success_rate += info['is_success']\n",
    "        local_success_rate = success_rate/num_tests\n",
    "        global_success_rate = MPI.COMM_WORLD.allreduce(local_success_rate, op=MPI.SUM)\n",
    "        return global_success_rate/MPI.COMM_WORLD.Get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b58d4e0-45e3-46ce-a2d5-8756e1e84d79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%file main.py\n",
    "import numpy as np\n",
    "import gym\n",
    "import os, sys\n",
    "from mpi4py import MPI\n",
    "from DDPG_HER import DDPG_HER\n",
    "import random\n",
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "    os.environ['MKL_NUM_THREADS'] = '1'\n",
    "    os.environ['IN_MPI'] = '1'\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--env-name', type=str, default='FetchReach-v1', help='the environment name')\n",
    "    parser.add_argument('--strategy', type=str, default='vanilla', help='sampling method')\n",
    "    parser.add_argument('--n-epochs', type=int, default=50, help='the number of epochs to train the agent')\n",
    "    parser.add_argument('--cuda', action='store_true', help='if use gpu do the acceleration')\n",
    "    parser.add_argument('--filename', type=str, default='ddpg', help='pickle file name to store object')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    env = gym.make(args.env_name)\n",
    "    seed = 123\n",
    "    np.random.seed(seed + MPI.COMM_WORLD.Get_rank())\n",
    "    torch.manual_seed(seed + MPI.COMM_WORLD.Get_rank())\n",
    "    env.seed(seed + MPI.COMM_WORLD.Get_rank())\n",
    "    random.seed(seed + MPI.COMM_WORLD.Get_rank())\n",
    "    torch.cuda.manual_seed(seed + MPI.COMM_WORLD.Get_rank())\n",
    "    \n",
    "    if MPI.COMM_WORLD.Get_rank() == 0:\n",
    "        print('Current Environment:',args.env_name)\n",
    "        print('Current Method:',args.strategy)\n",
    "    env.reset()\n",
    "\n",
    "    filename = args.filename+str(MPI.COMM_WORLD.Get_rank())+'.pickle'\n",
    "\n",
    "    ddpg_her_model = DDPG_HER(env,num_epochs = args.n_epochs,strategy = args.strategy,\\\n",
    "                             use_cuda = args.cuda)\n",
    "    start = time.time()\n",
    "    ddpg_her_model.train()\n",
    "    print('Training took',time.time()-start,'seconds')\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(ddpg_her_model,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeead25-f66a-4d41-8626-f5734ec9c12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Environment: FetchPush-v1\n",
      "Current Method: vanilla\n",
      "Epoch: 0 Avg Critic Loss: 0.005189810669689905 Avg Actor Loss: 0.31690679892105983 Success Rate: 0.0625\n",
      "Epoch: 1 Avg Critic Loss: 0.0034313689566333777 Avg Actor Loss: 0.6614896542578935 Success Rate: 0.0625\n",
      "Epoch: 2 Avg Critic Loss: 0.003829998292538221 Avg Actor Loss: 1.0463304397761821 Success Rate: 0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 8 python -u main.py --env-name 'FetchPush-v1' --filename='vanilla_her_push'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
